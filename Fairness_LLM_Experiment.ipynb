{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809271f-8105-4242-9067-5d5372d59cdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T09:54:46.093476667Z",
     "start_time": "2025-02-14T09:54:46.090159582Z"
    }
   },
   "outputs": [],
   "source": [
    "#Helps me keep the classes of Bias in Bios straight\n",
    "professions_dict = {\n",
    "    0: \"accountant\",\n",
    "    1: \"architect\",\n",
    "    2: \"attorney\",\n",
    "    3: \"chiropractor\",\n",
    "    4: \"comedian\",\n",
    "    5: \"composer\",\n",
    "    6: \"dentist\",\n",
    "    7: \"dietitian\",\n",
    "    8: \"dj\",\n",
    "    9: \"filmmaker\",\n",
    "    10: \"interior_designer\",\n",
    "    11: \"journalist\",\n",
    "    12: \"model\",\n",
    "    13: \"nurse\",\n",
    "    14: \"painter\",\n",
    "    15: \"paralegal\",\n",
    "    16: \"pastor\",\n",
    "    17: \"personal_trainer\",\n",
    "    18: \"photographer\",\n",
    "    19: \"physician\",\n",
    "    20: \"poet\",\n",
    "    21: \"professor\",\n",
    "    22: \"psychologist\",\n",
    "    23: \"rapper\",\n",
    "    24: \"software_engineer\",\n",
    "    25: \"surgeon\",\n",
    "    26: \"teacher\",\n",
    "    27: \"yoga_teacher\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904afe5-aa17-40db-9010-cf9799bc3483",
   "metadata": {},
   "source": [
    "## Embed data and train a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656360e3-b66f-4ee9-bffa-34dd88a40c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from embedding import BertHuggingface\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import scipy\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from concept_helpers.bert_cockatiel import BertCockatielWrapper\n",
    "from concept_helpers.cockatiel_sub import SubCockatiel\n",
    "from cockatiel.cockatiel import occlusion_concepts, print_legend, viz_concepts\n",
    "\n",
    "from experiment_helpers.experiment_helper_functions import *\n",
    "from experiment_helpers.lm_experiment_helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4b25d-bfbb-436e-98e4-b97721124809",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"LabHC/bias_in_bios\")\n",
    "\n",
    "text_train = dataset['train']['hard_text']\n",
    "y_train = dataset['train']['profession']\n",
    "gender_train = dataset['train']['gender']\n",
    "text_test = dataset['test']['hard_text']\n",
    "y_test = dataset['test']['profession']\n",
    "gender_test = dataset['test']['gender']\n",
    "\n",
    "NUM_CLASSES = np.max(y_test)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3055b1-2321-4c24-9fb4-015ac2a184c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "bert = BertCockatielWrapper(NUM_CLASSES, model_name=MODEL_NAME, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8dc7f-b9fd-40b1-8f74-dfde267ebf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ('models/finetuned_model_%s_relu' % MODEL_NAME)\n",
    "emb_savefile = ('Experiment3_results/embeddings_finetuned_%s_relu.pickle' % MODEL_NAME)\n",
    "\n",
    "if os.path.isdir(model_checkpoint):\n",
    "    print(\"load model from checkpoint\")\n",
    "    bert.load(model_checkpoint)\n",
    "else:\n",
    "    print(\"train and save model\")\n",
    "    bert.retrain(text_train, y_train, epochs=2)\n",
    "    bert.save(model_checkpoint)\n",
    "\n",
    "\n",
    "if os.path.isfile(emb_savefile):\n",
    "    print(\"load embeddings\")\n",
    "    with open(emb_savefile, 'rb') as handle:\n",
    "        embeddings = pickle.load(handle)\n",
    "    emb_train = embeddings['train']\n",
    "    emb_test = embeddings['test']\n",
    "\n",
    "    assert len(emb_test) == len(text_test)\n",
    "    assert len(emb_train) == len(text_train)\n",
    "else:\n",
    "    print(\"compute and save embeddings\")\n",
    "    emb_test = bert.embed(text_test)\n",
    "    emb_train = bert.embed(text_train)\n",
    "    embeddings = {'train': emb_train, 'test': emb_test}\n",
    "    with open(emb_savefile, 'wb') as handle:\n",
    "        pickle.dump(embeddings, handle)\n",
    "\n",
    "pred_file = ('Experiment3_results/test_predictions_%s_relu.npy' % MODEL_NAME)\n",
    "if os.path.isfile(pred_file):\n",
    "    y_pred = np.load(pred_file)\n",
    "else:\n",
    "    pred = bert.predict(list(text_test))\n",
    "    y_pred = np.argmax(pred, axis=1)\n",
    "    np.save(pred_file, y_pred)\n",
    "\n",
    "assert (emb_test >= 0).all()\n",
    "assert (emb_train >= 0).all()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed32ef-ff27-4b85-88cd-2f98550376b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bert F1-macro score: %.2f\" % f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8fd76f-a44d-4373-9cd5-607f5aead0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(activations, gender, class_id):\n",
    "    rf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    rf.fit(activations, gender)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    cm = plt.get_cmap('tab20')\n",
    "    plot_bars(ax, rf.feature_importances_, cm, (\"RF concept importances for gender prediction (class %i)\" % class_id))\n",
    "\n",
    "def get_pearsoncorr(activations, gender):\n",
    "    rs = []\n",
    "    for c in range(activations.shape[1]):\n",
    "        res = scipy.stats.pearsonr(gender, activations[:,c])\n",
    "        #print(\"concept %i got R=%.3f, p=%.3f\" % (c, res.statistic, res.pvalue))\n",
    "        rs.append(res.statistic)\n",
    "    return rs\n",
    "\n",
    "def plot_bars(ax, values, colormap, title):\n",
    "    ax.bar(range(len(values)), values, color=colormap.colors, tick_label=range(len(values)))\n",
    "    ax.set_title(title, fontsize=18)\n",
    "\n",
    "\n",
    "def sample_by_class_id(emb, text, y, gender, class_id, multiclass=False):\n",
    "    if multiclass:\n",
    "        assert type(class_id) == list\n",
    "        sample = []\n",
    "        for cid in class_id:\n",
    "            sample_c = np.where(y == cid)[0]\n",
    "            print(\"for class %i got %i samples\" % (cid, sample_c.shape[0]))\n",
    "            sample.append(sample_c)\n",
    "        sample = np.hstack(sample)\n",
    "    else:\n",
    "        assert type(class_id) == int\n",
    "        sample = np.where(y == class_id)[0]\n",
    "    sample_emb = np.array(emb)[sample]\n",
    "    sample_text = np.array(text)[sample]\n",
    "    sample_gender = np.array(gender)[sample]\n",
    "    return sample, sample_emb, sample_text, sample_gender\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac8c50-37dd-4647-b257-0557407c6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 physician\n",
    "class_id = 19\n",
    "multi_class = False\n",
    "\n",
    "n_concepts = 10\n",
    "class_lbl = professions_dict[class_id]\n",
    "#class_lbl = 'multiclass_med'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbe44a-a1c1-408c-bbc8-2887f5b0da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f863a-9077-4ddf-b540-2398fd3d1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_class_dir = (\"Experiment3_results/finetune_%s/\" % class_lbl)\n",
    "if not os.path.isdir(cur_class_dir):\n",
    "    os.makedirs(cur_class_dir)\n",
    "\n",
    "if multi_class:\n",
    "    print(\"choose multiple classes: \", class_id)\n",
    "else:\n",
    "    print(\"chose class %i (%s)\" % (class_id, class_lbl))\n",
    "sample, sample_emb, sample_text, sample_gender = sample_by_class_id(emb_test, text_test, y_pred, gender_test, class_id=class_id, multiclass=multi_class)\n",
    "\n",
    "sample_emb = torch.from_numpy(sample_emb)\n",
    "sample_emb = sample_emb.to(device)\n",
    "print(\"got %i samples\" % len(sample_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4be9cc-f104-4deb-b296-eb9ff96ef050",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_prob = 0.3\n",
    "#initialize model mc dropout model\n",
    "stochastic_model = StochasticModel(bert.model.classifier, dropout_prob=dropout_prob).to(device)\n",
    "\n",
    "#predict output dist\n",
    "predictions = predict_with_uncertainty_batched(stochastic_model.to(device), sample_emb, n_iter=100)\n",
    "predictions = predictions.cpu().numpy()\n",
    "\n",
    "#get uncertainty \n",
    "a, prob_mat = uncertainty_matrices(predictions)\n",
    "t, e, a = entropy_uncertainty(prob_mat)\n",
    "\n",
    "# sets threshold to separate uncertainty groups\n",
    "threshold, _, t_norm = get_threshold(t)\n",
    "\n",
    "# localizes the uncertainty (maps the uncertaintty values to a binary probability dist) close to threshold get .5 prob far away is close to 0 or 1\n",
    "loc = UncertaintyWrapperWithSigmoid(threshold)\n",
    "unc_pred_probs = loc.predict_proba(t_norm)\n",
    "unc_preds = np.argmax(unc_pred_probs, axis=1)\n",
    "\n",
    "# filters for UnC classes\n",
    "l_indices = np.where(unc_preds == 0)[0]\n",
    "h_indices = np.where(unc_preds == 1)[0]\n",
    "\n",
    "# create excerpts\n",
    "l_drift_articles = sample_text[l_indices]\n",
    "h_drift_articles = sample_text[h_indices]\n",
    "\n",
    "excerpt_dataset = excerpt_fct(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d669298-66c2-4d4a-bed5-aade7f7425cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map predictions, labels and prob/label of Stochastic model to excerpts\n",
    "\n",
    "def remove_first_occurrence(original_string, substring):\n",
    "    # Find the first occurrence of the substring\n",
    "    index = original_string.find(substring)\n",
    "    \n",
    "    # If the substring is found, remove it\n",
    "    if index != -1:\n",
    "        # Create a new string without the first occurrence of the substring\n",
    "        new_string = original_string[:index] + original_string[index + len(substring):]\n",
    "        return new_string\n",
    "    \n",
    "    # If the substring is not found, return the original string\n",
    "    return original_string\n",
    "\n",
    "excerpt_labels = []\n",
    "excerpt_pred = []\n",
    "unc_excerpt_labels = []\n",
    "unc_excerpt_probs = []\n",
    "\n",
    "cur_sample_id = 0\n",
    "cur_sample = sample_text[cur_sample_id]\n",
    "for i, excerpt in enumerate(excerpt_dataset):\n",
    "    if excerpt in cur_sample:\n",
    "        excerpt_labels.append(y_test[sample[cur_sample_id]])\n",
    "        excerpt_pred.append(y_pred[sample[cur_sample_id]])\n",
    "        unc_excerpt_probs.append(unc_pred_probs[cur_sample_id][1])\n",
    "        unc_excerpt_labels.append(unc_preds[cur_sample_id])\n",
    "        cur_sample = remove_first_occurrence(cur_sample, excerpt)\n",
    "    else:\n",
    "        cur_sample_id += 1\n",
    "        cur_sample = sample_text[cur_sample_id]\n",
    "        if not excerpt in cur_sample:\n",
    "            cur_sample_id += 1\n",
    "            cur_sample = sample_text[cur_sample_id]\n",
    "        \n",
    "        if excerpt in cur_sample:\n",
    "            excerpt_labels.append(y_test[sample[cur_sample_id]])\n",
    "            excerpt_pred.append(y_pred[sample[cur_sample_id]])\n",
    "            unc_excerpt_probs.append(unc_pred_probs[cur_sample_id][1])\n",
    "            unc_excerpt_labels.append(unc_preds[cur_sample_id])\n",
    "            cur_sample = remove_first_occurrence(cur_sample, excerpt)\n",
    "        else:\n",
    "            print(\"ERROR excerpt neither in current nor two next sample\")\n",
    "            excerpt_labels.append(-1)\n",
    "            excerpt_pred.append(-1) ###\n",
    "            unc_excerpt_probs.append(-1)\n",
    "            unc_excerpt_labels.append(-1)\n",
    "\n",
    "excerpt_labels = np.asarray(excerpt_labels)\n",
    "excerpt_pred = np.asarray(excerpt_pred)\n",
    "unc_excerpt_labels = np.asarray(unc_excerpt_labels)\n",
    "unc_excerpt_probs = np.asarray(unc_excerpt_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecdab5-acc5-43f5-919e-d0723fd61c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_indices_excerpt = np.where(unc_excerpt_labels == 0)[0]\n",
    "h_indices_excerpt = np.where(unc_excerpt_labels == 1)[0]\n",
    "\n",
    "excerpt_dataset_l = np.array(excerpt_dataset)[l_indices_excerpt].tolist()\n",
    "excerpt_dataset_h = np.array(excerpt_dataset)[h_indices_excerpt].tolist()\n",
    "\n",
    "print(len(excerpt_dataset_l), ' low unc excerpts created.')\n",
    "print(len(excerpt_dataset_h), ' high unc excerpts created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0d1a2-3205-4e31-b34c-4a73230b2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(l_indices))\n",
    "print(len(h_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c487188-984f-403d-81e9-42c79e2246cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the excerpts\n",
    "excerpt_file = 'Experiment3_results/finetune_excerpts_%s.pickle' % class_lbl\n",
    "\n",
    "max_n_samples = 1000\n",
    "max_n_excerpts = 1000\n",
    "\n",
    "if os.path.isfile(excerpt_file):\n",
    "    print(\"load excerpts and their embeddings\")\n",
    "    with open(excerpt_file, 'rb') as handle:\n",
    "        savedict = pickle.load(handle)\n",
    "\n",
    "    excerpt_samples = savedict['excerpts']\n",
    "    n_excerpts = savedict['n_excerpts']\n",
    "    excerpt_sample_ids = savedict['excerpt_sample_ids']\n",
    "    emb_excerpt = savedict['embeddings']\n",
    "\n",
    "    excerpt_low_ids = np.array(range(n_excerpts))\n",
    "    excerpt_high_ids = np.array(range(n_excerpts)) + n_excerpts\n",
    "    excerpt_samples_l = excerpt_samples[:n_excerpts]\n",
    "    print(len(excerpt_samples_l))\n",
    "    excerpt_samples_h = excerpt_samples[n_excerpts:]\n",
    "    print(len(excerpt_samples_h))\n",
    "\n",
    "    emb_excerpt_l = emb_excerpt[excerpt_low_ids,:]\n",
    "    emb_excerpt_h = emb_excerpt[excerpt_high_ids,:]\n",
    "\n",
    "    n_samples = savedict['n_samples']\n",
    "\n",
    "    sample_ids_l = savedict['sample_ids_l']\n",
    "    sample_ids_h = savedict['sample_ids_h']\n",
    "    \n",
    "    sample_emb_for_imp_l = sample_emb[sample_ids_l]\n",
    "    sample_emb_for_imp_h = sample_emb[sample_ids_h]\n",
    "\n",
    "else:\n",
    "    print(\"sample excerpts\")\n",
    "    n_excerpts = np.min([len(excerpt_dataset_l), len(excerpt_dataset_h), max_n_excerpts])\n",
    "    excerpt_low_ids = np.array(range(n_excerpts))\n",
    "    excerpt_high_ids = np.array(range(n_excerpts)) + n_excerpts\n",
    "    \n",
    "    excerpt_samples_ids_l = random.sample(excerpt_low_ids.tolist(), n_excerpts)\n",
    "    excerpt_samples_ids_h = random.sample(excerpt_high_ids.tolist(), n_excerpts)\n",
    "\n",
    "    excerpt_samples_l = np.asarray(excerpt_dataset)[excerpt_samples_ids_l].tolist()\n",
    "    excerpt_samples_h = np.asarray(excerpt_dataset)[excerpt_samples_ids_h].tolist()\n",
    "\n",
    "    excerpt_sample_ids = excerpt_samples_ids_l + excerpt_samples_ids_h\n",
    "    excerpt_samples = excerpt_samples_l + excerpt_samples_h\n",
    "    \n",
    "    print(\"embed %i excerpts...\" % (len(excerpt_samples)))\n",
    "    emb_excerpt = bert.embed(excerpt_samples)\n",
    "    emb_excerpt = relu(torch.from_numpy(emb_excerpt)).detach().numpy()\n",
    "    \n",
    "    emb_excerpt_l = emb_excerpt[excerpt_low_ids,:]\n",
    "    emb_excerpt_h = emb_excerpt[excerpt_high_ids,:]\n",
    "\n",
    "    n_samples = np.min([len(l_indices),len(h_indices), max_n_samples])\n",
    "\n",
    "    sample_ids_l = random.sample(l_indices.tolist(), n_samples)\n",
    "    sample_ids_h = random.sample(h_indices.tolist(), n_samples)\n",
    "    \n",
    "    sample_emb_for_imp_l = sample_emb[sample_ids_l]\n",
    "    sample_emb_for_imp_h = sample_emb[sample_ids_h]\n",
    "    \n",
    "    savedict = {'excerpt_sample_ids': excerpt_sample_ids, 'excerpts': excerpt_samples, 'n_excerpts': n_excerpts, 'embeddings': emb_excerpt, \n",
    "                'sample_ids_l': sample_ids_l, 'sample_ids_h': sample_ids_h, 'n_samples': n_samples}\n",
    "\n",
    "    #savedict = {'excerpt_sample_ids': excerpt_sample_ids, 'excerpts': excerpt_samples, 'n_samples': n_samples, 'embeddings': emb_excerpt}\n",
    "    with open(excerpt_file, 'wb') as handle:\n",
    "        pickle.dump(savedict, handle)\n",
    "\n",
    "# need ndarray and torch tensor\n",
    "sample_emb_np = sample_emb.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9a54c-3753-4647-b723-207b48313850",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_concepts = 10\n",
    "savefile = 'Experiment3_results/finetune_results_%s_%s.pickle' % (class_lbl, n_concepts)\n",
    "\n",
    "len_data = len(emb_excerpt)\n",
    "if n_samples < 100:\n",
    "    len_samples = len(sample_text)\n",
    "else:\n",
    "    len_samples = len_data//10\n",
    "print(len_data, len_samples)\n",
    "if os.path.isfile(savefile):\n",
    "    with open(savefile, 'rb') as handle:\n",
    "        results = pickle.load(handle)\n",
    "    \n",
    "    low_unc_importances = results['imp_low']\n",
    "    high_unc_importances = results['imp_high']\n",
    "    segments = results['segments']\n",
    "    u_segments = results['u_segments']\n",
    "    cockatiel_explainer = results['cockatiel']\n",
    "    factorization = results['factorization']\n",
    "    global_importance = results['global_importance']\n",
    "    _pa = results['_pa']\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"compute NMF and concept importances..\")\n",
    "    \n",
    "    # NMF for current class\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cockatiel_explainer = SubCockatiel(bert, bert.tokenizer, n_concepts, 64, device)\n",
    "        segments, u_segments, factorization, global_importance ,_pa = cockatiel_explainer.extract_concepts(\n",
    "                                                        excerpt_dataset[:len_data],\n",
    "                                                        np.asarray(emb_excerpt[:len_data]), \n",
    "                                                        sample_text[:(len_samples)],\n",
    "                                                        sample_emb_np[:(len_samples)], \n",
    "                                                        0, limit_sobol = 1_000)\n",
    "\n",
    "    #global importances over whole class, low and high uncertainty samples\n",
    "    print(\"compute high uncertainty concept importance...\")\n",
    "    high_unc_importances, high_sens_dict = estimate_importance(cockatiel_explainer, stochastic_model, sample_emb_for_imp_h, 1, factorization.components_)\n",
    "    print(\"compute low uncertainty concept importance...\")\n",
    "    low_unc_importances, low_sens_dict = estimate_importance(cockatiel_explainer, stochastic_model, sample_emb_for_imp_l, 0, factorization.components_)\n",
    "    #importances, sens_dict = estimate_importance(cockatiel_explainer, stochastic_model, sample_emb,1, factorization.components_)\n",
    "\n",
    "    results = {'imp_low': low_unc_importances, 'imp_high': high_unc_importances, 'segments': segments, 'u_segments': u_segments, 'cockatiel': cockatiel_explainer, 'factorization': factorization, 'global_importance': global_importance , '_pa': _pa}\n",
    "    \n",
    "    with open(savefile, 'wb') as handle:\n",
    "        pickle.dump(results, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0db4a-d4e1-475a-b9d7-7fd9ff00349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute concept activations\n",
    "activations_all = activation_transform(torch.from_numpy(emb_test), factorization.components_)\n",
    "activations_class = activation_transform(sample_emb, factorization.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b84eb-fd88-4592-a1f2-e2bf41ca5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation and feature importance for gender\n",
    "plot_feature_importance(activations_class, sample_gender, class_id)\n",
    "plt.savefig(\"%s/gender_concept_importance.png\" % cur_class_dir)\n",
    "\n",
    "rs_all = get_pearsoncorr(activations_all, gender_test)\n",
    "rs_class = get_pearsoncorr(activations_class, sample_gender)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 5))\n",
    "cm = plt.get_cmap('tab20')\n",
    "\n",
    "plot_bars(axes[0], rs_all, cm, \"Pearson correlation of concept activations with gender labels (all classes)\")\n",
    "plot_bars(axes[1], rs_class, cm, \"Pearson correlation of concept activations with gender labels (%s)\" % class_lbl)\n",
    "plt.savefig(\"%s/gender_corr.png\" % cur_class_dir)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e45c2-c844-48dc-bee0-0f3853c17b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "cm = plt.get_cmap('tab20')\n",
    "\n",
    "plot_bars(ax, rs_all, cm, \"Pearson correlation of concept activations with gender labels\")\n",
    "plt.ylabel('Pearson R', fontsize=14)\n",
    "plt.xlabel('Concepts', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"%s/gender_corr_.png\" % cur_class_dir)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a0824-0ca1-4c0a-8dec-199c57d7ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance for low/ high uncertainty\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "cm = plt.get_cmap('tab20')\n",
    "\n",
    "plot_bars(axes[0], low_unc_importances[0], cm, \"Low uncertainty samples (%s)\" % class_lbl)\n",
    "plot_bars(axes[1], high_unc_importances[0], cm, \"High uncertainty samples (%s)\" % class_lbl)\n",
    "\n",
    "axes[0].set_ylabel('Global concept importance', fontsize=14)\n",
    "axes[0].set_xlabel('Concepts', fontsize=14)\n",
    "axes[1].set_xlabel('Concepts', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"%s/unc_importance.png\" % cur_class_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a4042-cba8-48ae-a9f2-0deca896cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"low unc female ratio: \", np.sum(sample_gender[l_indices])/len(l_indices))\n",
    "print(\"high unc female ratio: \", np.sum(sample_gender[h_indices])/len(h_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea51820-006f-417b-983c-b9dd0125a063",
   "metadata": {},
   "source": [
    "## Explain concepts by token attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c625b-db66-4c3d-971d-1626597c7a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbated_activations = activation_transform(sample_emb, factorization.components_)\n",
    "perturbated_activations = perturbated_activations @ factorization.components_\n",
    "\n",
    "err = np.linalg.norm(sample_emb.cpu().detach().numpy() - perturbated_activations, 'fro')\n",
    "print(\"reconstruction error\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bda869-b4b3-4b7b-acd9-cf61f6ba384c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819d15e-2f72-4ca3-b72f-be23eca3a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_sel = [6]\n",
    "\n",
    "perturbated_activations = activation_transform(sample_emb, factorization.components_)\n",
    "perturbated_activations_rem = activation_transform(sample_emb, factorization.components_)\n",
    "for concept_id in concepts_sel:\n",
    "    print(\"set concept %i to its mean=%.3f\" % (concept_id, np.mean(perturbated_activations[:,concept_id])))\n",
    "    perturbated_activations_rem[:,concept_id] = np.mean(perturbated_activations_rem[:,concept_id])\n",
    "    \n",
    "perturbated_activations = perturbated_activations @ factorization.components_\n",
    "perturbated_activations_rem = perturbated_activations_rem @ factorization.components_\n",
    "\n",
    "err = np.linalg.norm(sample_emb.cpu().detach().numpy() - perturbated_activations, 'fro')\n",
    "print(\"reconstruction error: \", err)\n",
    "err = np.linalg.norm(sample_emb.cpu().detach().numpy() - perturbated_activations_rem, 'fro')\n",
    "print(\"reconstruction error + concept change: \", err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8379d-7b86-4ffe-9609-40ec3ea41f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batched(f_model, inputs, device=\"cuda\"):\n",
    "    inputs = inputs.to(device)\n",
    "    data_loader = torch.utils.data.DataLoader(inputs, batch_size=64)\n",
    "\n",
    "    preds = []\n",
    "    for inputs in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_preds = f_model(inputs)\n",
    "            # print(batch_preds)\n",
    "        if len(batch_preds.size()) == 1:\n",
    "            preds.append(batch_preds.reshape((1,batch_preds.shape[0])))\n",
    "        else:\n",
    "            preds.append(batch_preds)\n",
    "\n",
    "    # Stack predictions across iterations\n",
    "    return torch.vstack(preds)#, dim=0)\n",
    "\n",
    "pred = predict_batched(bert.model.classifier, torch.from_numpy(perturbated_activations))\n",
    "pred = pred.cpu().detach().numpy()\n",
    "y_pred_perturbed = np.argmax(pred, axis=1)\n",
    "\n",
    "\n",
    "pred = predict_batched(bert.model.classifier, torch.from_numpy(perturbated_activations_rem))\n",
    "pred = pred.cpu().detach().numpy()\n",
    "y_pred_perturbed_rem = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c8a1c-70a4-43cb-b124-845871a6cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%i predictions changed by reconstruction\" % (np.sum(y_pred[sample] != y_pred_perturbed)))\n",
    "print(\"%i predictions changed by concept removal\" % (np.sum(y_pred[sample] != y_pred_perturbed_rem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8711aa-136b-481c-ad0b-f3c7dc53d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_change = np.array(y_test)[sample][y_pred[sample] != y_pred_perturbed]\n",
    "y_pred_change = y_pred_perturbed[y_pred[sample] != y_pred_perturbed]\n",
    "gender_change = sample_gender[y_pred[sample] != y_pred_perturbed]\n",
    "is_low = [1 if idx in l_indices else 0 for idx in range(len(sample))]\n",
    "is_low_change = np.array(is_low)[y_pred[sample] != y_pred_perturbed]\n",
    "l_indices_change = np.where(is_low_change == 1)[0]\n",
    "h_indices_change = np.where(is_low_change == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41cc31-5602-43f7-bfd0-1253b0d7341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_change_rem = np.array(y_test)[sample][y_pred[sample] != y_pred_perturbed_rem]\n",
    "y_pred_change_rem = y_pred_perturbed_rem[y_pred[sample] != y_pred_perturbed_rem]\n",
    "gender_change_rem = sample_gender[y_pred[sample] != y_pred_perturbed_rem]\n",
    "is_low_rem = [1 if idx in l_indices else 0 for idx in range(len(sample))]\n",
    "is_low_change_rem = np.array(is_low_rem)[y_pred[sample] != y_pred_perturbed_rem]\n",
    "l_indices_change_rem = np.where(is_low_change_rem == 1)[0]\n",
    "h_indices_change_rem = np.where(is_low_change_rem == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b1654-120f-4b24-95dc-b63f155010f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%i predictions were corrected\" % np.sum(y_true_change == y_pred_change))\n",
    "print(\"%i predictions are still wrong\" % np.sum((y_true_change != y_pred_change) & (y_true_change != class_id)))\n",
    "print(\"%i predictions turned wrong\" % np.sum((y_true_change != y_pred_change) & (y_true_change == class_id)))\n",
    "\n",
    "print(\"percentage of high uncertainty samples among changed predictions: %.3f\" % (len(h_indices_change)/(len(h_indices_change)+len(l_indices_change))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53482bb7-a39a-41f5-a593-d79b532cf788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%i predictions were corrected\" % np.sum(y_true_change_rem == y_pred_change_rem))\n",
    "print(\"%i predictions are still wrong\" % np.sum((y_true_change_rem != y_pred_change_rem) & (y_true_change_rem != class_id)))\n",
    "print(\"%i predictions turned wrong\" % np.sum((y_true_change_rem != y_pred_change_rem) & (y_true_change_rem == class_id)))\n",
    "\n",
    "print(\"percentage of high uncertainty samples among changed predictions: %.3f\" % (len(h_indices_change)/(len(h_indices_change)+len(l_indices_change))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f42df-335e-4010-9db2-f8b5ce0ef714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ea7c7-fc4e-4f98-806e-1ea60c54a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"high uncertainty:\")\n",
    "print(\"%i predictions were corrected\" % np.sum((y_true_change == y_pred_change)[h_indices_change]))\n",
    "print(\"%i predictions are still wrong\" % np.sum(((y_true_change != y_pred_change) & (y_true_change != class_id))[h_indices_change]))\n",
    "print(\"%i predictions turned wrong\" % np.sum(((y_true_change != y_pred_change) & (y_true_change == class_id))[h_indices_change]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12be34b-7caf-4bd5-8255-999eadcd1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"low uncertainty:\")\n",
    "print(\"%i predictions were corrected\" % np.sum((y_true_change == y_pred_change)[l_indices_change]))\n",
    "print(\"%i predictions are still wrong\" % np.sum(((y_true_change != y_pred_change) & (y_true_change != class_id))[l_indices_change]))\n",
    "print(\"%i predictions turned wrong\" % np.sum(((y_true_change != y_pred_change) & (y_true_change == class_id))[l_indices_change]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c965b6-aec5-463a-b6f6-0a2f012f6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"female ratio among correct samples: \", np.mean(gender_change[y_true_change == y_pred_change]))\n",
    "print(\"female ratio among wrong-turned samples: \", np.mean(gender_change[(y_true_change != y_pred_change) & (y_true_change == class_id)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3cb7e-0448-4b87-bc7a-c344f6721b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"female ratio of class %i (true label): %.3f\" % (class_id, np.mean(np.array(gender_test)[np.array(y_test) == class_id])))\n",
    "print(\"female ratio of class %i predictions: %.3f\" % (class_id, np.mean(np.array(gender_test)[y_pred == class_id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a67e21-418b-4b7d-b356-a635bbcd62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"female ratio among prof->physician (bc of concept %i: %.3f\" % (concept_id, np.mean(gender_change[(y_true_change == y_pred_change) & (y_true_change == 21)])))\n",
    "print(\"female ratio among surgeon->physician (bc of concept %i: %.3f\" % (concept_id, np.mean(gender_change[(y_true_change == y_pred_change) & (y_true_change == 25)])))\n",
    "print(\"female ratio among chiro->physician (bc of concept %i: %.3f\" % (concept_id, np.mean(gender_change[(y_true_change == y_pred_change) & (y_true_change == 3)])))\n",
    "print(\"female ratio among teacher->physician (bc of concept %i: %.3f\" % (concept_id, np.mean(gender_change[(y_true_change == y_pred_change) & (y_true_change == 26)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ff21f-d84e-4fde-a1c9-3c24e3de42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cid in [21, 25, 3, 26]:\n",
    "    print(\"gender ratio of class %i (%s): %.3f\" % (cid, professions_dict[cid], np.mean(np.array(gender_test)[np.array(y_test) == cid])))\n",
    "    print(\"FP of class %i: %i\" % (cid, np.sum((y_true_change == y_pred_change) & (y_true_change == cid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cdb162-002e-4874-bbde-f8cd302a2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_true_change[(y_true_change == y_pred_change)], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b026ccf-2750-49c5-bf45-33774503b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%i predictions changed\" % (np.sum(y_pred[sample] != y_pred_perturbed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd3031-4aa1-4af1-bee1-d126b016a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_change = sample_gender[(y_pred[sample] != y_pred_perturbed)]\n",
    "print(\"female ratio of changed samples: \", np.sum(g_change)/len(g_change))\n",
    "\n",
    "g_prev_error = sample_gender[(y_pred[sample] != np.array(y_test)[sample])]\n",
    "g_perturbed_error = sample_gender[(y_pred_perturbed != np.array(y_test)[sample])]\n",
    "print(\"female ratio of previous errors: \", np.sum(g_prev_error)/len(g_prev_error))\n",
    "print(\"female ratio of errors after concept removal: \", np.sum(g_perturbed_error)/len(g_perturbed_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83d95a-8892-4196-aaa5-8aa18ede157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_neg_sample = [y_test[i] for i in range(len(y_test)) if i not in sample]\n",
    "y_pred_neg_sample = [y_pred[i] for i in range(len(y_pred)) if i not in sample]\n",
    "gender_neg_sample = [gender_test[i] for i in range(len(gender_test)) if i not in sample]\n",
    "\n",
    "y_test_all_pert_order = y_test_neg_sample + np.array(y_test)[sample].tolist()\n",
    "y_pred_all_pert_order = y_pred_neg_sample + y_pred_perturbed.tolist()\n",
    "y_pred_all_pert_order_rem = y_pred_neg_sample + y_pred_perturbed_rem.tolist()\n",
    "gender_all_pert_order = gender_neg_sample + np.array(gender_test)[sample].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8995b5e-d9cc-4eca-92f6-a06c70fdb49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: changed to FP because that makes more sense with cockatiel sampling\n",
    "def equalized_odds(y_true, y_pred, gender):\n",
    "    if type(y_true) == list:\n",
    "        y_true = np.array(y_true)\n",
    "    if type(y_pred) == list:\n",
    "        y_pred = np.array(y_pred)\n",
    "    if type(gender) == list:\n",
    "        gender = np.array(gender)\n",
    "        \n",
    "    n_classes = max(y_true)+1\n",
    "    scores = []\n",
    "    tp_count = []\n",
    "    for c in range(n_classes):\n",
    "        tp_mask = (y_pred[y_true == c] == c)\n",
    "        gender_c = gender[y_true == c]\n",
    "\n",
    "        tp_mask_0 = tp_mask[gender_c == 0]\n",
    "        tp_mask_1 = tp_mask[gender_c == 1]\n",
    "\n",
    "        n_samples_0 = tp_mask_0.shape[0]\n",
    "        n_samples_1 = tp_mask_1.shape[0]\n",
    "            \n",
    "        if n_samples_0 == 0 or n_samples_1 == 0:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            tp_0 = np.sum(tp_mask_0)/n_samples_0\n",
    "            tp_1 = np.sum(tp_mask_1)/n_samples_1\n",
    "            scores.append(tp_1 - tp_0)\n",
    "        tp_count.append(np.sum(tp_mask_0) + np.sum(tp_mask_1))\n",
    "\n",
    "    return scores, tp_count\n",
    "\n",
    "eo_base, tp_base = equalized_odds(y_test, y_pred, gender_test)\n",
    "eo_sample_recon, tp_recon = equalized_odds(y_test_all_pert_order,y_pred_all_pert_order, gender_all_pert_order)\n",
    "eo_sample_pert, tp_pert = equalized_odds(y_test_all_pert_order, y_pred_all_pert_order_rem, gender_all_pert_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477b899-965d-459d-b8e1-87328cdc3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"baseline: \", np.mean(eo_base))\n",
    "print(\"after reconstruction: \", np.mean(eo_sample_recon))\n",
    "print(\"after concept removal: \", np.mean(eo_sample_pert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ceadef-a4d5-4ab6-8318-f4bad60b3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(professions_dict.values())\n",
    "\n",
    "eo_diff = np.array(eo_base)-np.array(eo_sample_pert)\n",
    "eo_base = np.array(eo_base)[eo_diff != 0]\n",
    "eo_sample_pert = np.array(eo_sample_pert)[eo_diff != 0]\n",
    "eo_sample_recon = np.array(eo_sample_recon)[eo_diff != 0]\n",
    "\n",
    "eo_diff_sel = eo_diff[eo_diff != 0]\n",
    "class_names_sel = np.array(class_names)[eo_diff != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b3f84-7ce9-4bf7-b748-6d6ac293e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(eo_sample_recon)-np.abs(eo_sample_pert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e94aa-e293-4c0d-b155-2f78c5653018",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(eo_sample_recon)-np.abs(eo_sample_pert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5da78-2e6b-4fe9-ab78-6fdd99a2e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(eo_base))\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.2\n",
    "\n",
    "# Erstellen einer einzigen Figur und Achse\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "# Plotten des ersten Balkensatzes\n",
    "ax.bar(x - bar_width, eo_base, width=bar_width, label='Baseline', color='b')\n",
    "\n",
    "# Plotten des zweiten Balkensatzes\n",
    "ax.bar(x, eo_sample_recon, width=bar_width, label='After concept reconstruction', color='r')\n",
    "\n",
    "# Plotten des dritten Balkensatzes\n",
    "ax.bar(x + bar_width, eo_sample_pert, width=bar_width, label='After concept perturbation', color='g')\n",
    "\n",
    "# Set the x-ticks, labels, and rotation\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names_sel, rotation=90)\n",
    "ax.set_xlabel('Classes', fontsize=14)\n",
    "ax.set_ylabel('Equalized Odds', fontsize=14)\n",
    "ax.set_title('Equalized Odds Comparison', fontsize=18)\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig('%s/eq_odds.png' % cur_class_dir)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67c91c-ff12-47a8-8183-9362e8da2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"baseline: \", np.mean(np.abs(eo_base)))\n",
    "print(\"after reconstruction: \", np.mean(np.abs(eo_sample_recon)))\n",
    "print(\"after concept removal: \", np.mean(np.abs(eo_sample_pert)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ea378-7bfa-4c53-858e-64fb7a20ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all_pert_order_best = y_pred_neg_sample + np.array(y_test)[sample].tolist()\n",
    "eo_sample_best, tp_best = equalized_odds(y_test_all_pert_order, y_pred_all_pert_order_best, gender_all_pert_order)\n",
    "\n",
    "eo_sample_best = np.array(eo_sample_best)[eo_diff != 0]\n",
    "print(\"baseline: \", np.mean(np.abs(eo_base)))\n",
    "print(\"best possible EO from this intervention: \", np.mean(np.abs(eo_sample_best)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b0de9-07ea-45c5-ae0e-8abd4a3d77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(eo_sample_best)) - np.mean(np.abs(eo_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605471ba-7ac6-41ea-bb21-27818f118195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples that changed due to feature removal\n",
    "\n",
    "corrected_fp_mask = ((y_pred[sample] != y_pred_perturbed_rem) & (y_pred[sample] == y_pred_perturbed) & (np.array(y_test)[sample] == y_pred_perturbed_rem))\n",
    "new_fn_mask = ((y_pred[sample] == np.array(y_test)[sample]) & (y_pred_perturbed_rem != y_pred_perturbed) & (np.array(y_test)[sample] != y_pred_perturbed_rem))\n",
    "\n",
    "sent_corrected_fp = sample_text[corrected_fp_mask]\n",
    "sent_new_fn = sample_text[new_fn_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20daee6-1289-48e1-b346-32b6be954191",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"professors classified as physician: \", np.sum((y_pred == 19) & (np.array(y_test) == 21)))\n",
    "print(\"professors in the test data: \", np.sum(np.array(y_test) == 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd5bc7-7251-457f-92ca-13c974c6636f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ec79a-cda2-4f9c-920e-370a2f697ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bec9bf09-7f59-4c45-8801-81f4aaa9e7c3",
   "metadata": {},
   "source": [
    "## Explain concepts by token attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c1e07-4f3e-402b-9163-9e16bae00408",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imgkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b217a3-a4e2-42d1-8ff4-f324c60cf31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgkit\n",
    "from typing import List, Optional\n",
    "from IPython.core.display import display, HTML\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cockatiel.cockatiel.utils import extract_clauses\n",
    "nltk.download('punkt')\n",
    "\n",
    "phi_thresh = 0.05\n",
    "\n",
    "def viz_concepts_to_img(\n",
    "        output_image_path,\n",
    "        text,\n",
    "        explanation,\n",
    "        colors,\n",
    "        ignore_words: Optional[List[str]] = None,\n",
    "        extract_fct: str = \"clause\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates the visualization for COCKATIEL's explanations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text\n",
    "        A string with the text we wish to explain.\n",
    "    explanation\n",
    "        An array that corresponds to the output of the occlusion function.\n",
    "    ignore_words\n",
    "        A list of strings to ignore when applying occlusion.\n",
    "    extract_fct\n",
    "        A string indicating whether at which level we wish to explain: \"word\", \"clause\" or \"sentence\".\n",
    "    colors\n",
    "        A dictionary with the colors for each label\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = text.decode('utf-8')\n",
    "    except:\n",
    "        text = str(text)\n",
    "\n",
    "    if extract_fct == \"clause\":\n",
    "        words = extract_clauses(text, clause_type=None)\n",
    "    else:\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "    l_phi = np.array(explanation)\n",
    "\n",
    "    phi_html = []\n",
    "\n",
    "    p = 0  # pointer to get current color for the words (it does not color words that have no phi)\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in ignore_words:\n",
    "            k = 0\n",
    "            for j in range(len(l_phi)):\n",
    "                if l_phi[k][p] < l_phi[j][p]:\n",
    "                    k = j\n",
    "\n",
    "            if l_phi[k][p] > phi_thresh:\n",
    "                phi_html.append(f'<span style=\"background-color: {colors[k]} {l_phi[k][p]}); padding: 1px 5px; border: solid 3px ; border-color: {colors[k]} 1); #EFEFEF\">{words[i]}</span>')\n",
    "                p += 1\n",
    "            else:\n",
    "                phi_html.append(f'<span style=\"background-color: rgba(233,30,99,0);  padding: 1px 5px; border: solid 3px ; border-color:  rgba(233,30,99,0); #EFEFEF\">{words[i]}</span>')\n",
    "                p += 1\n",
    "        else:\n",
    "            phi_html.append(f'<span style=\"background-color: rgba(233,30,99,0);  padding: 1px 5px; border: solid 3px ; border-color:  rgba(233,30,99,0); #EFEFEF\">{words[i]}</span>')\n",
    "    #display(HTML(\"<div style='display: flex; width: 400px; flex-wrap: wrap'>\" +  \" \".join(phi_html) + \" </div>\" ))\n",
    "    #display(HTML('<br><br>'))\n",
    "\n",
    "    complete_html = f\"<div style='display: flex; width: 400px; flex-wrap: wrap'>{' '.join(phi_html)}</div>\"\n",
    "    imgkit.from_string(complete_html, output_image_path)\n",
    "\n",
    "def print_legend_img(output_image_path, colors, label_to_criterion):\n",
    "    \"\"\"\n",
    "    Prints the legend for the plot in different colors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    colors\n",
    "        A dictionary with the colors for each label.\n",
    "    label_to_criterion\n",
    "        A dictionary with the text to put on each label.\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for label_id in label_to_criterion.keys():\n",
    "        html.append(f'<span style=\"background-color: {colors[label_id]} 0.5); padding: 1px 5px; border: solid 3px ; border-color: {colors[label_id]} 1); #EFEFEF\">{label_to_criterion[label_id]} </span>')\n",
    "    #display(HTML(\"<div style='display: flex; width: 400px; flex-wrap: wrap'>\" +  \" \".join(html) + \" </div>\" ))\n",
    "    #display(HTML('<br><br>'))\n",
    "    \n",
    "    complete_html = f\"<div style='display: flex; width: 400px; flex-wrap: wrap'>{' '.join(html)}</div>\"\n",
    "    imgkit.from_string(complete_html, output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb3801-99db-4d5c-b522-5e20e164a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from cockatiel (to adjust the threshold of phi)\n",
    "\n",
    "def viz_concepts(\n",
    "        text,\n",
    "        explanation,\n",
    "        colors,\n",
    "        ignore_words: Optional[List[str]] = None,\n",
    "        extract_fct: str = \"clause\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates the visualization for COCKATIEL's explanations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text\n",
    "        A string with the text we wish to explain.\n",
    "    explanation\n",
    "        An array that corresponds to the output of the occlusion function.\n",
    "    ignore_words\n",
    "        A list of strings to ignore when applying occlusion.\n",
    "    extract_fct\n",
    "        A string indicating whether at which level we wish to explain: \"word\", \"clause\" or \"sentence\".\n",
    "    colors\n",
    "        A dictionary with the colors for each label\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = text.decode('utf-8')\n",
    "    except:\n",
    "        text = str(text)\n",
    "\n",
    "    if extract_fct == \"clause\":\n",
    "        words = extract_clauses(text, clause_type=None)\n",
    "    else:\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "    l_phi = np.array(explanation)\n",
    "\n",
    "    phi_html = []\n",
    "\n",
    "    p = 0  # pointer to get current color for the words (it does not color words that have no phi)\n",
    "    for i in range(len(words)):\n",
    "        if words[i] not in ignore_words:\n",
    "            k = 0\n",
    "            for j in range(len(l_phi)):\n",
    "                if l_phi[k][p] < l_phi[j][p]:\n",
    "                    k = j\n",
    "\n",
    "            if l_phi[k][p] > phi_thresh:\n",
    "                phi_html.append(f'<span style=\"background-color: {colors[k]} {l_phi[k][p]}); padding: 1px 5px; border: solid 3px ; border-color: {colors[k]} 1); #EFEFEF\">{words[i]}</span>')\n",
    "                p += 1\n",
    "            else:\n",
    "                phi_html.append(f'<span style=\"background-color: rgba(233,30,99,0);  padding: 1px 5px; border: solid 3px ; border-color:  rgba(233,30,99,0); #EFEFEF\">{words[i]}</span>')\n",
    "                p += 1\n",
    "        else:\n",
    "            phi_html.append(f'<span style=\"background-color: rgba(233,30,99,0);  padding: 1px 5px; border: solid 3px ; border-color:  rgba(233,30,99,0); #EFEFEF\">{words[i]}</span>')\n",
    "    display(HTML(\"<div style='display: flex; width: 400px; flex-wrap: wrap'>\" +  \" \".join(phi_html) + \" </div>\" ))\n",
    "    display(HTML('<br><br>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee066aa-9f72-4735-80c2-97efcdd84734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = {\n",
    "    0: \"rgba(9, 221, 55, \",   # green\n",
    "    1: \"rgba(9, 221, 161, \",  # turquoise\n",
    "    2: \"rgba(9, 175, 221, \",  # blue\n",
    "    3: \"rgba(221, 9, 34, \",   # red\n",
    "    4: \"rgba(221, 9, 140, \",  # pink\n",
    "    5: \"rgba(221, 90, 9, \",   # orange\n",
    "    6: \"rgba(221, 9, 221, \",  # bright red\n",
    "    7: \"rgba(221, 221, 9, \",  # yellow\n",
    "    8: \"rgba(9, 55, 221, \",   # \n",
    "    9: \"rgba(9, 221, 9, \",    # lime\n",
    "}\n",
    "\n",
    "tokenizer = bert.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd45138-622a-4213-bfc4-6c4a64738228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to False to display the token attribution instead of saving images\n",
    "SAVE_IMG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325b71e-7c1b-46b5-9917-b98257dc67e9",
   "metadata": {},
   "source": [
    "### false-positives with high uncertainty (fixed by concept removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f95f0-40d3-4d6f-81ce-50e06da7a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high uncertainty\n",
    "i = 0\n",
    "n = len(sent_corrected_fp)\n",
    "m_pos = 4\n",
    "l_concept_id_l = [2,9,4,6] #[2,3,7,9,4,6]\n",
    "\n",
    "label_to_criterion = {idx: {'id': l_concept_id_l[idx], 'label': \"Positive label: concept %i\" % l_concept_id_l[idx]} for idx in range(m_pos)}\n",
    "\n",
    "\n",
    "colors_to_draw = {idx: colors[idx] for idx in range(m_pos)}\n",
    "label_criterion_for_legend = {idx: label_to_criterion[idx]['label'] for idx in range(m_pos)}\n",
    "l_concept_ids = [idx for idx in l_concept_id_l]\n",
    "next_idx = m_pos\n",
    "\n",
    "#sanity check:\n",
    "if len(label_criterion_for_legend.keys()) != len(colors_to_draw.keys()) or len(label_criterion_for_legend.keys()) > (m_pos):\n",
    "    print(\"Error: check that you have the correct number of colors and labels in your dictionaries to cover \\\n",
    "    the number of concepts being looked at\")\n",
    "\n",
    "if SAVE_IMG:\n",
    "    print_legend_img(('%s/legend.png' % cur_class_dir), colors_to_draw, label_criterion_for_legend)\n",
    "else:\n",
    "    print_legend(colors_to_draw, label_criterion_for_legend)\n",
    "\n",
    "\n",
    "for sentence in sent_corrected_fp:\n",
    "    if i%n == 0 :\n",
    "        print(\"\\n\")\n",
    "        print(\"samples that are falsely predicted as physician due to concept 6:\")\n",
    "        print(\"\\n\")\n",
    "    phi = occlusion_concepts(sentence, bert, tokenizer, factorization, \n",
    "                           l_concept_ids, ignore_words = [], two_labels = False, device = device)\n",
    "    phi /= np.max(np.abs(phi)) + 1e-5\n",
    "    \n",
    "    if SAVE_IMG:\n",
    "        img_name = '%s/sent_corrected_fp_%i.png' % (cur_class_dir, i)\n",
    "        viz_concepts_to_img(img_name, sentence, phi, colors_to_draw, ignore_words = [])\n",
    "    else:\n",
    "        viz_concepts(sentence, phi, colors_to_draw, ignore_words = [])\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9352a71-ef45-44ab-bb37-fce2931f7f81",
   "metadata": {},
   "source": [
    "### true-positives with high-uncertainty (turned false-negative after concept removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3dea4a-5e75-4b92-9648-34a6fdc31995",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "n = len(sent_new_fn)\n",
    "for sentence in sent_new_fn:\n",
    "    if i%n == 0 :\n",
    "        print(\"\\n\")\n",
    "        print(\"samples that are falsely predicted as physician due to concept 6:\")\n",
    "        print(\"\\n\")\n",
    "    phi = occlusion_concepts(sentence, bert, tokenizer, factorization, \n",
    "                           l_concept_ids, ignore_words = [], two_labels = False, device = device)\n",
    "    phi /= np.max(np.abs(phi)) + 1e-5\n",
    "\n",
    "    if SAVE_IMG:\n",
    "        img_name = '%s/sent_new_fn_%i.png' % (cur_class_dir, i)\n",
    "        viz_concepts_to_img(img_name, sentence, phi, colors_to_draw, ignore_words = [])\n",
    "    else:\n",
    "        viz_concepts(sentence, phi, colors_to_draw, ignore_words = [])\n",
    "  \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f123c22-ebbe-4d20-96c4-ebbb2cadaecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c6e07-be42-43ce-8f9f-20b913e39073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
